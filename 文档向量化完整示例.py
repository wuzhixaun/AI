"""
æ–‡æ¡£å‘é‡åŒ–å®Œæ•´ç¤ºä¾‹ - åˆå­¦è€…æ•™ç¨‹
=====================================

è¿™ä¸ªç¤ºä¾‹å°†æ•™ä½ ï¼š
1. ä»€ä¹ˆæ˜¯æ–‡æ¡£å‘é‡åŒ–
2. å¦‚ä½•å°†é•¿æ–‡æ¡£åˆ†å‰²æˆå°å—
3. å¦‚ä½•å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡
4. å¦‚ä½•å­˜å‚¨å’Œæœç´¢å‘é‡

æ ¸å¿ƒæ¦‚å¿µï¼š
- æ–‡æ¡£åˆ†å‰²ï¼šå°†é•¿æ–‡æ¡£åˆ‡åˆ†æˆå°å—ï¼Œä¾¿äºå¤„ç†
- å‘é‡åŒ–ï¼šå°†æ–‡å­—è½¬æ¢æˆæ•°å­—å‘é‡ï¼Œä¿ç•™è¯­ä¹‰ä¿¡æ¯
- å‘é‡æ•°æ®åº“ï¼šå­˜å‚¨å‘é‡ï¼Œæ”¯æŒå¿«é€Ÿç›¸ä¼¼åº¦æœç´¢
"""

import os
import shutil
from dotenv import load_dotenv
from langchain_chroma import Chroma
from langchain_core.documents import Document
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

# åŠ è½½ç¯å¢ƒå˜é‡ï¼ˆä» .env æ–‡ä»¶è¯»å–é…ç½®ï¼‰
load_dotenv()

print("=" * 60)
print("ğŸ“š æ–‡æ¡£å‘é‡åŒ–å®Œæ•´ç¤ºä¾‹")
print("=" * 60)

# ============================================
# æ­¥éª¤1: åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ï¼ˆEmbedding Modelï¼‰
# ============================================
print("\nã€æ­¥éª¤1ã€‘åˆå§‹åŒ–å‘é‡åŒ–æ¨¡å‹...")
print("è¯´æ˜ï¼šåµŒå…¥æ¨¡å‹è´Ÿè´£å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å­—å‘é‡")

# ä»ç¯å¢ƒå˜é‡è¯»å–æ¨¡å‹è·¯å¾„
embedding_model_path = os.getenv("EMBEDDING_MODEL")

# åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
# HuggingFaceEmbeddings æ˜¯ä¸€ä¸ªé¢„è®­ç»ƒçš„æ¨¡å‹ï¼Œå¯ä»¥å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡
embeddings_model = HuggingFaceEmbeddings(model_name=embedding_model_path)
print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {embedding_model_path}")

# ============================================
# æ­¥éª¤2: å‡†å¤‡è¦å¤„ç†çš„æ–‡æ¡£
# ============================================
print("\nã€æ­¥éª¤2ã€‘å‡†å¤‡æ–‡æ¡£å†…å®¹...")
print("è¯´æ˜ï¼šè¿™æ˜¯æˆ‘ä»¬è¦å‘é‡åŒ–çš„åŸå§‹æ–‡æœ¬")

long_text = """
äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligenceï¼ŒAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚
è¿™äº›ä»»åŠ¡åŒ…æ‹¬å­¦ä¹ ã€æ¨ç†ã€é—®é¢˜è§£å†³ã€æ„ŸçŸ¥å’Œè¯­è¨€ç†è§£ã€‚

æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é›†ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿåœ¨æ²¡æœ‰è¢«æ˜ç¡®ç¼–ç¨‹çš„æƒ…å†µä¸‹å­¦ä¹ å’Œæ”¹è¿›ã€‚
æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é›†ï¼Œä½¿ç”¨äººå·¥ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å·¥ä½œæ–¹å¼ã€‚

è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ˜¯AIçš„å¦ä¸€ä¸ªé‡è¦åˆ†æ”¯ï¼Œä¸“æ³¨äºè®¡ç®—æœºä¸äººç±»è¯­è¨€ä¹‹é—´çš„äº¤äº’ã€‚
è¿™åŒ…æ‹¬æ–‡æœ¬ç†è§£ã€è¯­è¨€ç”Ÿæˆã€æœºå™¨ç¿»è¯‘å’Œæƒ…æ„Ÿåˆ†æç­‰ä»»åŠ¡ã€‚

è®¡ç®—æœºè§†è§‰ä½¿æœºå™¨èƒ½å¤Ÿä»æ•°å­—å›¾åƒæˆ–è§†é¢‘ä¸­è·å–æœ‰æ„ä¹‰çš„ä¿¡æ¯ã€‚
å®ƒåŒ…æ‹¬å›¾åƒè¯†åˆ«ã€ç‰©ä½“æ£€æµ‹ã€äººè„¸è¯†åˆ«å’Œå›¾åƒç”Ÿæˆç­‰æŠ€æœ¯ã€‚
"""

print(f"åŸå§‹æ–‡æ¡£é•¿åº¦: {len(long_text)} å­—ç¬¦")
print(f"æ–‡æ¡£å†…å®¹é¢„è§ˆ: {long_text[:100]}...")

# ============================================
# æ­¥éª¤3: æ–‡æ¡£åˆ†å‰²ï¼ˆDocument Splittingï¼‰
# ============================================
print("\nã€æ­¥éª¤3ã€‘æ–‡æ¡£åˆ†å‰²...")
print("è¯´æ˜ï¼šå°†é•¿æ–‡æ¡£åˆ‡åˆ†æˆå°å—ï¼ŒåŸå› ï¼š")
print("  1. æ¨¡å‹æœ‰è¾“å…¥é•¿åº¦é™åˆ¶")
print("  2. å°å—æ›´å®¹æ˜“å¤„ç†å’Œæœç´¢")
print("  3. å¯ä»¥æ›´ç²¾ç¡®åœ°å®šä½ç›¸å…³ä¿¡æ¯")

# åˆ›å»ºæ–‡æ¡£åˆ†å‰²å™¨
# chunk_size: æ¯ä¸ªå—çš„æœ€å¤§å­—ç¬¦æ•°
# chunk_overlap: å—ä¹‹é—´çš„é‡å å­—ç¬¦æ•°ï¼ˆä¿æŒä¸Šä¸‹æ–‡è¿ç»­æ€§ï¼‰
# length_function: è®¡ç®—æ–‡æœ¬é•¿åº¦çš„å‡½æ•°
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=200,      # æ¯å—æœ€å¤š200ä¸ªå­—ç¬¦
    chunk_overlap=100,   # ç›¸é‚»å—é‡å 100ä¸ªå­—ç¬¦ï¼ˆé¿å…ä¿¡æ¯ä¸¢å¤±ï¼‰
    length_function=len  # ä½¿ç”¨lenå‡½æ•°è®¡ç®—é•¿åº¦
)

# æ‰§è¡Œåˆ†å‰²
chunks = text_splitter.split_text(long_text)

print(f"âœ… åˆ†å‰²å®Œæˆ!")
print(f"   åŸå§‹æ–‡æ¡£: {len(long_text)} å­—ç¬¦")
print(f"   åˆ†å‰²åå—æ•°: {len(chunks)}")
print(f"\nåˆ†å‰²ç»“æœé¢„è§ˆ:")
for i, chunk in enumerate(chunks, 1):
    print(f"   å— {i}: {chunk[:80]}...")

# ============================================
# æ­¥éª¤4: å°†æ–‡æœ¬å—è½¬æ¢ä¸ºDocumentå¯¹è±¡
# ============================================
print("\nã€æ­¥éª¤4ã€‘è½¬æ¢ä¸ºDocumentå¯¹è±¡...")
print("è¯´æ˜ï¼šDocumentæ˜¯LangChainçš„æ ‡å‡†æ ¼å¼ï¼ŒåŒ…å«å†…å®¹å’Œå…ƒæ•°æ®")

# å°†æ–‡æœ¬å—è½¬æ¢ä¸ºDocumentå¯¹è±¡
# å¯ä»¥æ·»åŠ å…ƒæ•°æ®ï¼ˆmetadataï¼‰ï¼Œæ¯”å¦‚æ¥æºã€æ ‡é¢˜ç­‰
documents = [
    Document(
        page_content=chunk,
        metadata={"chunk_id": i, "source": "AIçŸ¥è¯†åº“"}
    )
    for i, chunk in enumerate(chunks)
]

print(f"âœ… è½¬æ¢å®Œæˆ! å…± {len(documents)} ä¸ªDocumentå¯¹è±¡")
print(f"   ç¤ºä¾‹Document: {documents[0].page_content[:50]}...")
print(f"   å…ƒæ•°æ®: {documents[0].metadata}")

# ============================================
# æ­¥éª¤5: åˆ›å»ºå‘é‡æ•°æ®åº“ï¼ˆVector Storeï¼‰
# ============================================
print("\nã€æ­¥éª¤5ã€‘åˆ›å»ºå‘é‡æ•°æ®åº“...")
print("è¯´æ˜ï¼šå‘é‡æ•°æ®åº“ä¼šï¼š")
print("  1. è‡ªåŠ¨å°†æ‰€æœ‰æ–‡æ¡£å‘é‡åŒ–")
print("  2. å­˜å‚¨å‘é‡å’ŒåŸå§‹æ–‡æœ¬çš„æ˜ å°„å…³ç³»")
print("  3. æ”¯æŒå¿«é€Ÿç›¸ä¼¼åº¦æœç´¢")

# å…ˆæµ‹è¯•ä¸€ä¸‹å½“å‰æ¨¡å‹çš„å‘é‡ç»´åº¦
test_vector = embeddings_model.embed_query("æµ‹è¯•")
vector_dimension = len(test_vector)
print(f"   å½“å‰æ¨¡å‹çš„å‘é‡ç»´åº¦: {vector_dimension}")

# ä½¿ç”¨Chromaåˆ›å»ºå‘é‡æ•°æ®åº“
# persist_directory: æ•°æ®åº“ä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼Œä¸æŒ‡å®šåˆ™åªåœ¨å†…å­˜ä¸­ï¼‰
# embedding_function: ä½¿ç”¨çš„åµŒå…¥æ¨¡å‹
persist_dir = "./chroma_db"

# å¤„ç†ç»´åº¦ä¸åŒ¹é…çš„é—®é¢˜ï¼šå¦‚æœæ•°æ®åº“å·²å­˜åœ¨ä½†ç»´åº¦ä¸åŒ¹é…ï¼Œéœ€è¦åˆ é™¤é‡å»º
try:
    # å°è¯•åˆ›å»ºæˆ–åŠ è½½æ•°æ®åº“
    vector_db = Chroma.from_documents(
        documents=documents,
        embedding=embeddings_model,
        persist_directory=persist_dir
    )
    print("âœ… å‘é‡æ•°æ®åº“åˆ›å»ºæˆåŠŸ!")
except Exception as e:
    if "dimension" in str(e).lower() or "embedding" in str(e).lower():
        print(f"âš ï¸  æ£€æµ‹åˆ°ç»´åº¦ä¸åŒ¹é…é”™è¯¯: {e}")
        print("   åŸå› ï¼šå·²å­˜åœ¨çš„æ•°æ®åº“ä½¿ç”¨äº†ä¸åŒç»´åº¦çš„å‘é‡æ¨¡å‹")
        print("   è§£å†³æ–¹æ¡ˆï¼šåˆ é™¤æ—§æ•°æ®åº“ï¼Œé‡æ–°åˆ›å»º...")
        
        # åˆ é™¤æ—§çš„æ•°æ®åº“
        if os.path.exists(persist_dir):
            shutil.rmtree(persist_dir)
            print(f"   âœ… å·²åˆ é™¤æ—§æ•°æ®åº“: {persist_dir}")
        
        # é‡æ–°åˆ›å»ºæ•°æ®åº“
        vector_db = Chroma.from_documents(
            documents=documents,
            embedding=embeddings_model,
            persist_directory=persist_dir
        )
        print("âœ… å‘é‡æ•°æ®åº“é‡æ–°åˆ›å»ºæˆåŠŸ!")
    else:
        # å…¶ä»–é”™è¯¯ï¼Œé‡æ–°æŠ›å‡º
        raise

print(f"   æ•°æ®åº“ä½ç½®: {persist_dir}")
print(f"   å­˜å‚¨çš„æ–‡æ¡£æ•°: {len(documents)}")

# ============================================
# æ­¥éª¤6: å‘é‡æœç´¢ï¼ˆVector Searchï¼‰
# ============================================
print("\nã€æ­¥éª¤6ã€‘å‘é‡æœç´¢æ¼”ç¤º...")
print("è¯´æ˜ï¼šé€šè¿‡è®¡ç®—å‘é‡ç›¸ä¼¼åº¦ï¼Œæ‰¾åˆ°æœ€ç›¸å…³çš„æ–‡æ¡£ç‰‡æ®µ")

# å®šä¹‰å‡ ä¸ªæµ‹è¯•æŸ¥è¯¢
test_queries = [
    "ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ï¼Ÿ",
    "AIå¦‚ä½•å¤„ç†å›¾åƒï¼Ÿ",
    "æœºå™¨å­¦ä¹ çš„å®šä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ"
]

for query in test_queries:
    print(f"\nğŸ” æŸ¥è¯¢: '{query}'")
    print("-" * 50)
    
    # æ‰§è¡Œç›¸ä¼¼åº¦æœç´¢
    # k=2 è¡¨ç¤ºè¿”å›æœ€ç›¸ä¼¼çš„2ä¸ªç»“æœ
    results = vector_db.similarity_search(query, k=2)
    
    print(f"æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³ç»“æœ:")
    for i, doc in enumerate(results, 1):
        print(f"\n  ç»“æœ {i}:")
        print(f"    å†…å®¹: {doc.page_content[:100]}...")
        print(f"    å…ƒæ•°æ®: {doc.metadata}")

# ============================================
# æ­¥éª¤7: å¸¦ç›¸ä¼¼åº¦åˆ†æ•°çš„æœç´¢
# ============================================
print("\nã€æ­¥éª¤7ã€‘å¸¦ç›¸ä¼¼åº¦åˆ†æ•°çš„æœç´¢...")
print("è¯´æ˜ï¼šç›¸ä¼¼åº¦åˆ†æ•°è¶Šé«˜ï¼Œè¡¨ç¤ºè¶Šç›¸å…³ï¼ˆChromaä½¿ç”¨è·ç¦»ï¼Œè¶Šå°è¶Šç›¸ä¼¼ï¼‰")

query = "è‡ªç„¶è¯­è¨€å¤„ç†çš„åº”ç”¨"
print(f"\nğŸ” æŸ¥è¯¢: '{query}'")
print("-" * 50)

# ä½¿ç”¨ similarity_search_with_score è·å–ç›¸ä¼¼åº¦åˆ†æ•°
results_with_scores = vector_db.similarity_search_with_score(query, k=2)

for i, (doc, score) in enumerate(results_with_scores, 1):
    print(f"\n  ç»“æœ {i}:")
    print(f"    ç›¸ä¼¼åº¦è·ç¦»: {score:.4f} (è¶Šå°è¶Šç›¸ä¼¼)")
    print(f"    å†…å®¹: {doc.page_content}")

# ============================================
# æ­¥éª¤8: ç†è§£å‘é‡åŒ–çš„åŸç†
# ============================================
print("\nã€æ­¥éª¤8ã€‘ç†è§£å‘é‡åŒ–çš„åŸç†...")
print("è¯´æ˜ï¼šè®©æˆ‘ä»¬çœ‹çœ‹æ–‡æœ¬æ˜¯å¦‚ä½•è¢«è½¬æ¢æˆå‘é‡çš„")

# å°†æŸ¥è¯¢æ–‡æœ¬å‘é‡åŒ–
sample_text = "äººå·¥æ™ºèƒ½å’Œæœºå™¨å­¦ä¹ "
query_vector = embeddings_model.embed_query(sample_text)

print(f"\næ–‡æœ¬: '{sample_text}'")
print(f"å‘é‡ç»´åº¦: {len(query_vector)} ç»´")
print(f"å‘é‡å‰10ä¸ªå€¼: {query_vector[:10]}")
print("\nğŸ’¡ ç†è§£ï¼š")
print("   - æ¯ä¸ªæ–‡æœ¬éƒ½è¢«è½¬æ¢æˆäº†ä¸€ä¸ªå›ºå®šé•¿åº¦çš„æ•°å­—æ•°ç»„ï¼ˆå‘é‡ï¼‰")
print("   - ç›¸ä¼¼çš„æ–‡æœ¬ä¼šæœ‰ç›¸ä¼¼çš„å‘é‡å€¼")
print("   - é€šè¿‡è®¡ç®—å‘é‡é—´çš„è·ç¦»ï¼Œå¯ä»¥æ‰¾åˆ°è¯­ä¹‰ç›¸ä¼¼çš„æ–‡æœ¬")

# ============================================
# æ€»ç»“
# ============================================
print("\n" + "=" * 60)
print("ğŸ‰ æ–‡æ¡£å‘é‡åŒ–æµç¨‹å®Œæˆ!")
print("=" * 60)
print("\nğŸ“– æ€»ç»“ - ä½ å­¦åˆ°äº†ä»€ä¹ˆï¼š")
print("  1. âœ… æ–‡æ¡£åˆ†å‰²ï¼šå°†é•¿æ–‡æ¡£åˆ‡åˆ†æˆå°å—")
print("  2. âœ… å‘é‡åŒ–ï¼šä½¿ç”¨åµŒå…¥æ¨¡å‹å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡")
print("  3. âœ… å­˜å‚¨ï¼šå°†å‘é‡å­˜å…¥å‘é‡æ•°æ®åº“ï¼ˆChromaï¼‰")
print("  4. âœ… æœç´¢ï¼šé€šè¿‡å‘é‡ç›¸ä¼¼åº¦æ‰¾åˆ°ç›¸å…³å†…å®¹")
print("\nğŸš€ ä¸‹ä¸€æ­¥å­¦ä¹ ï¼š")
print("  - å°è¯•ä¸åŒçš„chunk_sizeå’Œchunk_overlapå‚æ•°")
print("  - å­¦ä¹ å¦‚ä½•ä½¿ç”¨å…¶ä»–å‘é‡æ•°æ®åº“ï¼ˆå¦‚FAISSã€Pineconeï¼‰")
print("  - äº†è§£RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰ç³»ç»Ÿå¦‚ä½•ç»“åˆå‘é‡æœç´¢å’Œå¤§æ¨¡å‹")
print("\nğŸ’¾ æç¤ºï¼šå‘é‡æ•°æ®åº“å·²ä¿å­˜åˆ° ./chroma_dbï¼Œä¸‹æ¬¡å¯ä»¥ç›´æ¥åŠ è½½ä½¿ç”¨")
print("=" * 60)